from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import requests
from bs4 import BeautifulSoup
import feedparser
from datetime import datetime, timedelta, timezone
import time
import re
from urllib.parse import urljoin
import json
from email_sender import NewsEmailSender

KST = timezone(timedelta(hours=9))

class NewsCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def crawl_bbc_headline(self):       
        try:
            url = "https://www.bbc.com/news"
            response = self.session.get(url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # BBC ë©”ì¸ ì»¨í…ì¸  ì˜ì—­
            main_content = soup.select_one("#main-content article section div div div")
            
            headline_link = main_content.select_one('a[href*="/news/"]')
            
            title = headline_link.get_text(strip=True)
            link = urljoin(url, headline_link.get('href', ''))
            
            print("BBC ë©”ì¸ í—¤ë“œë¼ì¸:")
            print(f"ì œëª©: {title}")
            print(f"ë§í¬: {link}")
            return {
                'title': title,
                'link': link
            }
            
        except Exception as e:
            print(f"BBC í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None
    
    def crawl_cnn_headline(self):       
        try:
            url = "https://www.cnn.com/"
            response = self.session.get(url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # CNN ë©”ì¸ í—¤ë“œë¼ì¸ - ì œê³µëœ êµ¬ì¡° ê¸°ë°˜
            headline_link = soup.select_one('.container__title--emphatic a')
            
            title_element = headline_link.select_one('h2') or headline_link
            title = title_element.get_text(strip=True)
            link = urljoin(url, headline_link.get('href', ''))
            
            print("CNN ë©”ì¸ í—¤ë“œë¼ì¸:")
            print(f"ì œëª©: {title}")
            print(f"ë§í¬: {link}")
            return {
                'title': title,
                'link': link
            }

            return None
            
        except Exception as e:
            print(f"CNN í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None
    
    def crawl_fox_headline(self):       
        try:
            url = "https://www.foxnews.com/"
            response = self.session.get(url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Fox News ë©”ì¸ í—¤ë“œë¼ì¸ - big-top ì˜ì—­ì˜ ì²« ë²ˆì§¸ ê¸°ì‚¬
            headline_link = soup.select_one('.big-top .content article .info-header h3.title a')
            
            title = headline_link.get_text(strip=True)
            link = urljoin(url, headline_link.get('href', ''))
            
            print("Fox News ë©”ì¸ í—¤ë“œë¼ì¸:")
            print(f"ì œëª©: {title}")
            print(f"ë§í¬: {link}")
            return {
                'title': title,
                'link': link
            }

            return None
            
        except Exception as e:
            print(f"Fox News í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None
    
    def crawl_nyt_headline(self):       
        try:
            url = "https://www.nytimes.com/"
            response = self.session.get(url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # NYT ë©”ì¸ í—¤ë“œë¼ì¸ - story-wrapper ì˜ì—­ì˜ ë§í¬
            headline_link = soup.select_one('.story-wrapper .indicate-hover')
            
            parent_link = headline_link.find_parent('a')
            title = headline_link.get_text(strip=True)
            link = urljoin(url, parent_link.get('href', ''))
            
            print("NYT ë©”ì¸ í—¤ë“œë¼ì¸:")
            print(f"ì œëª©: {title}")
            print(f"ë§í¬: {link}")
            return {
                'title': title,
                'link': link
            }
            
            return None
            
        except Exception as e:
            print(f"NYT í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None

    def crawl_wp_headline_selenium(self):
        """Seleniumìœ¼ë¡œ Washington Post ë©”ì¸ í—¤ë“œë¼ì¸ í¬ë¡¤ë§"""
        driver = None
        try:
            # Chrome ì˜µì…˜ ì„¤ì •
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # ë¸Œë¼ìš°ì € ì°½ ìˆ¨ê¸°ê¸°
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # WebDriver ì´ˆê¸°í™”
            driver = webdriver.Chrome(options=chrome_options)
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            url = "https://www.washingtonpost.com/"
            driver.get(url)
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            wait = WebDriverWait(driver, 10)
            
            # ìš”ì†Œê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
            element = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.headline h2 a span')))
            
            if element:
                title = element.text.strip()
                
                # ë¶€ëª¨ ë§í¬ ì°¾ê¸°
                link_element = element.find_element(By.XPATH, './ancestor::a[1]')
                link = urljoin(url, link_element.get_attribute('href'))
                
                print("Washington Post ë©”ì¸ í—¤ë“œë¼ì¸:")
                print(f"ì œëª©: {title}")
                print(f"ë§í¬: {link}")
                
                return {
                    'title': title,
                    'link': link
                }
            
        except Exception as e:
            print(f"âŒ WP Selenium í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None
            
        finally:
            if driver:
                driver.quit()

    def crawl_yonhap_request(self):
        """ì—°í•©ë‰´ìŠ¤ êµ­ì œ ê¸°ì‚¬ - requestsë¡œ ì›¹í˜ì´ì§€ì—ì„œ í¬ë¡¤ë§ (ìˆ˜ì •ëœ ë²„ì „)"""
        def parse_yonhap_time(time_text):
            """ì—°í•©ë‰´ìŠ¤ ì‹œê°„ í…ìŠ¤íŠ¸ íŒŒì‹± (09-07 20:52 í˜•ì‹)"""
            try:
                # í˜„ì¬ ì—°ë„ ê°€ì ¸ì˜¤ê¸°
                current_year = datetime.now().year
                
                # 09-07 20:52 í˜•ì‹ íŒŒì‹±
                if len(time_text.split()) == 2:
                    date_part, time_part = time_text.split()
                    
                    # ì›”-ì¼ ì‹œ:ë¶„ í˜•ì‹
                    if '-' in date_part and ':' in time_part:
                        month, day = date_part.split('-')
                        hour, minute = time_part.split(':')
                        
                        parsed_time = datetime(
                            year=current_year,
                            month=int(month),
                            day=int(day),
                            hour=int(hour),
                            minute=int(minute),
                            tzinfo=KST
                        )
                        
                        return parsed_time
                
                return None
                
            except Exception as e:
                print(f"ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {time_text}, ì˜¤ë¥˜: {e}")
                return None

        def get_yonhap_content_request(url):
            """ì—°í•©ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸° (requests ë°©ì‹)"""
            try:
                response = self.session.get(url)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                content_div = soup.select_one('#articleWrap')
                
                if content_div:
                    for unwanted in content_div.find_all(['script', 'style', 'aside', 'footer', 'nav']):
                        unwanted.decompose()
                    
                    text = content_div.get_text(strip=True)
                    return text
                
            except Exception as e:
                print(f"ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
                return None
                
        print("=== ì—°í•©ë‰´ìŠ¤ êµ­ì œ ê¸°ì‚¬ í…ŒìŠ¤íŠ¸ (requests) ===")
        try:
            # ì‹œê°„ í•„í„°ë§ (ì „ë‚  23:00 ~ ë‹¹ì¼ 08:30)
            now = datetime.now(KST)
            yesterday_23 = (now - timedelta(days=1)).replace(hour=23, minute=0, second=0, microsecond=0)
            today_0830 = now.replace(hour=8, minute=30, second=0, microsecond=0)
            
            print(f"í•„í„°ë§ ì‹œê°„: {yesterday_23} ~ {today_0830}")
            
            filtered_articles = []
            
            # 1í˜ì´ì§€ì™€ 2í˜ì´ì§€ í¬ë¡¤ë§
            for page in [1, 2, 3]:
                url = f"https://www.yna.co.kr/international/all/{page}"
                print(f"\ní˜ì´ì§€ {page} í¬ë¡¤ë§: {url}")
                
                response = self.session.get(url)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # div.section01 ì•ˆì˜ ë‰´ìŠ¤ ë§í¬ë“¤
                news_links = soup.select('div.section01 a.tit-news')

                # div.section01 ì•ˆì˜ ì‹œê°„ ìš”ì†Œë“¤  
                time_elements = soup.select('div.section01 span.txt-time')
                
                # ê° ë‰´ìŠ¤ ë§í¬ ì²˜ë¦¬
                for i, link in enumerate(news_links):
                    try:
                        title = link.get_text(strip=True)
                        href = link.get('href', '')
                        
                        # ì ˆëŒ€ URLë¡œ ë³€í™˜
                        full_link = urljoin('https://www.yna.co.kr', href)
                        
                        # ì‹œê°„ ì •ë³´ ì°¾ê¸°
                        article_time = None
                        time_text = time_elements[i].get_text(strip=True)
                        article_time = parse_yonhap_time(time_text)
                        
                        # ì‹œê°„ í•„í„°ë§
                        if yesterday_23 <= article_time <= today_0830:
                            filtered_articles.append({
                                'title': title,
                                'link': full_link,
                                'published': article_time
                            })
                            
                            print(f"\nğŸ“° ê¸°ì‚¬ {len(filtered_articles)}:")
                            print(f"ì œëª©: {title}")
                            print(f"ë°œí–‰ì‹œê°„: {article_time}")
                            print(f"ë§í¬: {full_link}")
                            content = get_yonhap_content_request(full_link)
                            print(f"ë³¸ë¬¸ ({len(content)}ì): {content}")

                    except Exception as e:
                        print(f"ê¸°ì‚¬ {i+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue
            
            print(f"\nâœ… ì‹œê°„ ë²”ìœ„ ë‚´ ì´ ê¸°ì‚¬ ìˆ˜: {len(filtered_articles)}")
            return filtered_articles
            
        except Exception as e:
            print(f"âŒ ì—°í•©ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("=== ì¼ì¼ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘ ===")
    
    # ê¸°ì¡´ í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    crawler = NewsCrawler()
    email_sender = NewsEmailSender()
    
    # ì™¸ì‹  í¬ë¡¤ë§
    foreign_news = {}
    foreign_sites = [
        ("BBC", crawler.crawl_bbc_headline),
        ("CNN", crawler.crawl_cnn_headline),
        ("Fox News", crawler.crawl_fox_headline),
        ("NYT", crawler.crawl_nyt_headline),
        ("Washington Post", crawler.crawl_wp_headline_selenium)
    ]
    
    for site_name, crawl_func in foreign_sites:
        try:
            print(f"\n{site_name} í¬ë¡¤ë§ ì¤‘...")
            result = crawl_func()
            foreign_news[site_name] = result
            time.sleep(2)
        except Exception as e:
            print(f"{site_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            foreign_news[site_name] = None
    
    # ì—°í•©ë‰´ìŠ¤ í¬ë¡¤ë§
    try:
        print(f"\nì—°í•©ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘...")
        yonhap_articles = crawler.crawl_yonhap_request()
    except Exception as e:
        print(f"ì—°í•©ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        yonhap_articles = []
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\n=== í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½ ===")
    print(f"ì™¸ì‹  ì„±ê³µ: {sum(1 for v in foreign_news.values() if v)}/{len(foreign_news)}")
    print(f"ì—°í•©ë‰´ìŠ¤ ê¸°ì‚¬: {len(yonhap_articles)}ê°œ")
    
    # ì´ë©”ì¼ ë°œì†¡
    print(f"\nì´ë©”ì¼ ë°œì†¡ ì¤‘...")
    success = email_sender.send_email(foreign_news, yonhap_articles)
    
    if success:
        print("ğŸ‰ ì¼ì¼ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ ì™„ë£Œ!")
    else:
        print("âŒ ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨")

if __name__ == "__main__":
    main()